{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Tree Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "import dtreeviz\n",
    "#also, must install graphviz for plotting to work with xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_center = 12\n",
    "pos_count = 100\n",
    "neg_center = 7\n",
    "neg_count = 1000\n",
    "rs = rn.RandomState(rn.MT19937(rn.SeedSequence(5)))\n",
    "gini = pd.DataFrame({'value': np.append((pos_center) + rs.randn(pos_count),\n",
    "                           (neg_center) + rs.randn(neg_count)), \n",
    "                    'label': ['pos']*pos_count + ['neg']*neg_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "_ = (gini\n",
    ".groupby('label')\n",
    "[['value']]\n",
    "    .plot.hist(bins=30, alpha=.5, ax=ax, edgecolor='black')\n",
    ")\n",
    "ax.legend(['Negative','Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(df: pd.DataFrame, val_col: str, label_col: str, pos_val, split_point: float, debug=False) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the Gini impurity of a dataset. Gini impurity is a measure of the probability of a random sample bine gclassified incorrectly when a feature is used to split the data. The lower the impurity, the better the split.\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    val_col (str): The column name of the feature used to split the data\n",
    "    label_col (str): The column name of the target variable\n",
    "    pos_val (str or int): The value of the target variable that represents the positive class\n",
    "    split_point (float): The threshold used to split the data.\n",
    "    debug (bool): optional, when set to True, prints the calculated Gini impurities and the final weighted average\n",
    "    \"\"\"\n",
    "    ge_split = df[val_col] >= split_point\n",
    "    eq_pos = df[label_col] == pos_val\n",
    "    tp = df[ge_split & eq_pos].shape[0] #num rows greater than split point.\n",
    "    fp = df[ge_split & ~eq_pos].shape[0]\n",
    "    tn = df[~ge_split & ~eq_pos].shape[0]\n",
    "    fn = df[~ge_split & eq_pos].shape[0]\n",
    "    pos_size = tp + fp\n",
    "    neg_size = tn + fn\n",
    "    total_size = len(df)\n",
    "    if pos_size == 0:\n",
    "        gini_pos = 0\n",
    "    else:\n",
    "        gini_pos = 1 - (tp/pos_size)**2 - (fp/pos_size)**2\n",
    "    if neg_size == 0:\n",
    "        gini_neg = 0\n",
    "    else:\n",
    "        gini_neg = 1 - (tn/neg_size)**2 - (fn/neg_size)**2\n",
    "    weighted_avg = gini_pos * (pos_size/total_size) + \\\n",
    "        gini_neg*(neg_size/total_size)\n",
    "    if debug:\n",
    "        print(f'{gini_pos=:.3} {gini_neg=:.3} {weighted_avg=:.3}') #the =:.3 is a precision specification. it says, specify to 3 sig figs\n",
    "    return weighted_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_gini(gini, val_col='value', label_col='label', pos_val='pos', split_point=9.24, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrating sig fig. if do it with blah = 100, get an error that no sig figs with ints\n",
    "blah = 100.01\n",
    "print(f'{blah=:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = np.arange(5,15,.1) #like, array range. From 5 to 15 by .1\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ginis = []\n",
    "for v in values: #so, splitting them for each of the values\n",
    "    ginis.append(calc_gini(gini, val_col='value', label_col = 'label', pos_val='pos', split_point=v))\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(values, ginis)\n",
    "ax.set_title('Gini Coefficient')\n",
    "ax.set_ylabel('Gini Coefficient')\n",
    "ax.set_xlabel('Split Point')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ginis, index=values).loc[9.5:10.5] #this doesn't include 9.5 but does 10.5. but values includes 9.5 for sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'gini':ginis, 'split':values}).query('gini<= gini.min()') #note that gini.min() didn't have to be wrapped with an f-string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump = tree.DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(gini[['value']], gini.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "tree.plot_tree(stump, feature_names=['value'], filled=True, class_names=stump.classes_, ax=ax) \n",
    "#note that tree.plot_tree is an sklearn object\n",
    "# note that the tree is split around 9.6 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_stump = xgb.XGBClassifier(n_estimators=1, max_depth=1)\n",
    "xg_stump.fit(gini[['value']], (gini.label=='pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(xgb.plot_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xg_stump, num_trees=0) #num_trees is the index of the tree. \n",
    "#xg_stump is a single decision tree, so only index=0 has one. \n",
    "#if we fit a random forest, there would be many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = dtreeviz.model(xg_stump, X_train=gini[['value']], y_train=gini.label=='pos',\n",
    "    target_name='positive',\n",
    "    feature_names=['value'],\n",
    "    class_names=['negative','positive'],\n",
    "    tree_index=0)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_stump.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(xg_stump)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
